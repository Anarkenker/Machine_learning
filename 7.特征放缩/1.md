# 特征放缩

特征放缩（Feature Scaling）是指通过某种方法将不同特征的取值范围进行调整，使它们处于相近的尺度。这样可以让模型训练时，各特征对结果的影响更加均衡，提升模型的收敛速度和效果。

![](7.特征放缩/image.png)

![](7.特征放缩/image-1.png)

常见的特征放缩方法有：
- **均值归一化（Mean Normalization）**  
    通过减去均值并除以取值范围，将特征缩放到 $[-1, 1]$ 左右。  
    $$
    x' = \frac{x - \mu}{x_{max} - x_{min}}
    $$
    其中 $\mu$ 是特征的均值，$x_{max}$ 和 $x_{min}$ 分别是最大值和最小值。

    均值归一化可以让特征的均值变为 $0$，并且缩放到一个较小的范围，有助于提升模型训练的效率和稳定性。

- **标准化（Z-score Standardization）**  
    将特征缩放为均值为 $0$，标准差为 $1$。  
    $$
    x' = \frac{x - \mu}{\sigma}
    $$
    其中 $\mu$ 是均值，$\sigma$ 是标准差。

**标准差**（Standard Deviation，记作 $\sigma$）是衡量一组数据离均值远近程度的统计量。它反映了数据的离散程度或波动大小。 

计算公式如下：
$$
\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2}
$$
其中：
- $x_i$ 表示第 $i$ 个数据，
- $\mu$ 表示所有数据的均值，
- $n$ 表示数据的总个数。

**标准差越大，说明数据分布越分散；标准差越小，说明数据越集中在均值附近。**

特征放缩的作用：
- 让不同量纲的特征具有可比性
- 加快模型收敛速度
- 提高模型精度

只需要改变特征的范围，就能让特征之间的变化更加明显。

举个例子：

假设有两个特征：房屋面积（$x_1$，单位：平方米）和房间数量（$x_2$，单位：个）：

| 样本 | 房屋面积 $x_1$ | 房间数量 $x_2$ |
|------|:-------------:|:-------------:|
| 1    | 120           | 3             |
| 2    | 80            | 2             |
| 3    | 200           | 5             |

**均值归一化（Mean Scaling）**：

以房屋面积为例，$x_{min}=80$，$x_{max}=200$，则第一个样本的归一化结果为：
$$
x_1' = \frac{120 - 80}{200 - 80} = \frac{40}{120} \approx 0.33
$$

**标准化（Z-score Standardization）**：

假设房屋面积的均值 $\mu=133.3$，标准差 $\sigma=61.2$，则第一个样本的标准化结果为：
$$
x_1' = \frac{120 - 133.3}{61.2} \approx -0.22
$$

通过特征放缩，面积和房间数这两个特征就变成了相近的数值范围，更有利于模型训练。

一般来说，**归一化（Min-Max Scaling）** 和 **标准化（Z-score Standardization）** 的选择可以根据以下情况：

### 什么时候用归一化（Mean Scaling）？
- 当特征的分布**不是正态分布**，而且希望把所有特征缩放到固定区间（如 $[0, 1]$）时。
- 在需要保持**原始数据分布形状**，但只是缩放范围的场景下。
- 常用于**图像处理**等数据本身有明确上下界的场景。

### 什么时候用标准化（Z-score Standardization）？
- 当特征的分布**接近正态分布**（高斯分布）时，标准化效果更好。
- 在算法对**异常值不敏感**或对分布有要求时（如线性回归、逻辑回归、SVM、K均值等）。
- 当数据中有**异常值**时，标准化比归一化更稳健，因为归一化容易受极端值影响。

---

**总结：**
- 数据分布接近正态分布时，优先用标准化；
- 数据分布无明显规律或有明确取值范围时，优先用归一化；
- 实际应用中可以尝试两种方法，选择对模型效果更好的那一个。

## 特征放缩对梯度下降的作用

特征放缩可以显著提升梯度下降（Gradient Descent）算法的效率和效果，主要体现在以下几个方面：

1. **加快收敛速度**  
   如果各特征的取值范围差异很大，梯度下降时参数更新的步伐会在不同方向上差异很大，导致收敛速度变慢。特征放缩后，各特征处于相近的尺度，梯度下降能更快地找到最优解。

2. **避免“锯齿”式下降**  
   未放缩时，等高线呈现“扁长”椭圆，梯度下降路径会“之”字形前进，效率低。放缩后，等高线趋于圆形，下降路径更直接。

3. **提升数值稳定性**  
   特征放缩可以避免某些特征值过大导致的数值不稳定或溢出问题，使模型训练更加稳定。

4. **让学习率更容易选择**  
   特征放缩后，统一的学习率对所有参数都适用，无需为不同尺度的特征单独调整学习率。

**总结:**  
特征放缩能让梯度下降算法更快、更稳定地收敛到最优解，是实际建模中非常重要的一步。

![](7.特征放缩/image-2.png)

**如何判断特征是否需要放缩**：

- 一般建议每个特征 $x_j$ 的取值范围在 $[-1, 1]$ 或 $[-3, 3]$ 之间，甚至 $[-0.3, 0.3]$ 也可以接受。
- 如果特征的取值范围在这些区间内（如 $x_1$ 和 $x_2$），可以不用放缩（图中标注 okay, no rescaling）。
- 如果特征的范围太大（如 $x_3$ 在 $[-100, 100]$），或者太小（如 $x_4$ 在 $[-0.001, 0.001]$），就需要进行放缩（图中标注 too large/too small → rescale）。
- $x_5$ 的范围（$98.6$ 到 $105$）虽然跨度不大，但绝对值较大，也建议放缩。

**总结：**  
只要特征的取值范围远大于或远小于 $[-1, 1]$，就建议进行特征放缩，使所有特征处于相近的尺度，有利于模型训练和收敛。